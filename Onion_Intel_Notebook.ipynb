{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onion Intel\n",
    "\n",
    "This is the first version of my tool, it's just a scratch. \n",
    "\n",
    "That's why I choose to use a Jupyter notebook so I can debug every step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T17:58:41.628795Z",
     "start_time": "2024-08-14T17:58:41.569199Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import re\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying if the Ahmia site is online. I chose to start with Ahmia because it provides access to its database of .onion sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T17:58:43.543500Z",
     "start_time": "2024-08-14T17:58:41.595561Z"
    }
   },
   "outputs": [],
   "source": [
    "ahmia=\"https://ahmia.fi/onions/\"\n",
    "response = requests.get(ahmia)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Ahmia is on!\")\n",
    "    urls_onion = response.content\n",
    "elif response.status_code == 404:\n",
    "    print(\"Ahmia is down!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the Ahmia Database stored in the `/onions/` path. \n",
    "\n",
    "> The HTML needs to be processed to extract only the .onion site URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T17:58:45.335435Z",
     "start_time": "2024-08-14T17:58:43.692303Z"
    }
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "# Split the content into lines and filter out empty lines\n",
    "lines = soup.text.splitlines()\n",
    "non_empty_lines = [line.strip() for line in lines if line.strip() != \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving it into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T17:58:45.401540Z",
     "start_time": "2024-08-14T17:58:45.334972Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the cleaned lines to a file\n",
    "with open(\"ahmia.txt\", \"w\") as file:\n",
    "    for line in non_empty_lines:\n",
    "        file.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the new entries with the previous ones to ensure checks are performed only on newly discovered sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T17:58:45.545748Z",
     "start_time": "2024-08-14T17:58:45.388376Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the entries from anemia.txt\n",
    "with open(\"ahmia.txt\", \"r\") as file:\n",
    "    ahmia_entries = set(line.strip() for line in file if line.strip())\n",
    "\n",
    "# Read the entries from ahmia_old.txt\n",
    "with open(\"ahmia_old.txt\", \"r\") as file:\n",
    "    ahmia_old_entries = set(line.strip() for line in file if line.strip())\n",
    "\n",
    "# Find the new entries in anemia.txt that are not in ahmia_old.txt\n",
    "new_entries = ahmia_entries - ahmia_old_entries\n",
    "\n",
    "# Save the new entries to a file (e.g., new_entries.txt)\n",
    "with open(\"new_entries.txt\", \"w\") as file:\n",
    "    for entry in new_entries:\n",
    "        file.write(entry + \"\\n\")\n",
    "\n",
    "print(f\"Found {len(new_entries)} new entries.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the file ahmia_old.txt is updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T17:58:45.741033Z",
     "start_time": "2024-08-14T17:58:45.543720Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine old and new entries\n",
    "combined_entries = ahmia_old_entries.union(new_entries)\n",
    "\n",
    "# Save the combined entries back to ahmia_old.txt\n",
    "with open(\"ahmia_old.txt\", \"w\") as file:\n",
    "    for entry in sorted(combined_entries):\n",
    "        file.write(entry + \"\\n\")\n",
    "\n",
    "print(f\"Updated 'ahmia_old.txt' with {len(new_entries)} new entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case TOR is already running this part is not necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T17:58:45.771969Z",
     "start_time": "2024-08-14T17:58:45.582694Z"
    }
   },
   "outputs": [],
   "source": [
    "# Start Tor using subprocess\n",
    "tor_process = subprocess.Popen(['tor'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!netstat -tupan | grep 9050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!netstat -tupan | grep 9053"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for debuging process\n",
    "#tor_process.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTPX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kali Linux already includes a binary named `httpx`. Therefore, `PDTM` is used to accurately locate the actual installation path of `httpx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the PDTM command and capture both stdout and stderr\n",
    "process = subprocess.Popen('pdtm', shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "stdout, stderr = process.communicate()\n",
    "\n",
    "# Convert the output to string and combine stdout and stderr\n",
    "output = stdout.decode('utf-8') + stderr.decode('utf-8')\n",
    "\n",
    "# Find the line containing the path\n",
    "match = re.search(r'Path to download project binary: (.*)', output)\n",
    "\n",
    "# Extract the path and store it in a variable\n",
    "if match:\n",
    "    pdtm_path = match.group(1)\n",
    "    print(f\"PDTM Path: {pdtm_path}\")\n",
    "else:\n",
    "    print(\"Path not found in the output.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if new_entries.txt exists and is readable\n",
    "if os.path.isfile(\"new_entries.txt\"):\n",
    "    print(\"new_entries.txt exists and is readable.\")\n",
    "else:\n",
    "    print(\"new_entries.txt is missing or not readable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jupyter Notebook allows execution of shell commands using `!`, which is how HTTPX was run within the notebook.\n",
    "\n",
    "The HTTPX then creates a CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the httpx command directly in Jupyter\n",
    "!{pdtm_path}/httpx -l new_entries.txt --proxy socks5://127.0.0.1:9050 -timeout 50 --title -ss -esb -ehb -silent -follow-redirects -csv -o new_entries_httpx.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the data frame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSV file created by HTTPX, was used to create an DataFrame with the Pandas Librarie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Use pandas to read the CSV file\n",
    "csv_file = \"new_entries_httpx.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(csv_file, encoding='ISO-8859-1')\n",
    "    #print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"File {csv_file} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the CSV contained numerous columns, many of which were unnecessary. A new DataFrame was created with only the selected columns of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = df [['timestamp','port','url','input','title','webserver','content_type','method','host','path','tech','words','lines','status_code','content_length','stored_response_path','screenshot_path_rel']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'stored_response_path_rel' column by extracting the relative path\n",
    "df_base['stored_response_path_rel'] = df['stored_response_path'].apply(lambda x: x.split('OnionIntel/output/response/')[-1] if pd.notnull(x) else '')\n",
    "# Drop the old 'stored_response_path' column\n",
    "df_base = df_base.drop(columns=['stored_response_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new column was created, named `Onion_Site`, because it will be used to merge the dataframe created by `NMAP` later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create the 'Onion_Site' column by extracting the .onion part of the 'url' column\n",
    "df_base['Onion_Site'] = df_base['url'].apply(lambda url: urlparse(url).netloc if urlparse(url).netloc.endswith(\".onion\") else None)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df_base[['url', 'Onion_Site']].head())  # Displaying only the 'url' and 'Onion_Site' columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving all onion sites into a file, so it can be sent to NMAP scan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a new file to store the .onion URLs\n",
    "onion_file = \"new_entries_onion_sites.txt\"\n",
    "\n",
    "with open(onion_file, \"w\") as file:\n",
    "    # Loop over the DataFrame\n",
    "    for index, row in df_base.iterrows():\n",
    "        # Extract the URL\n",
    "        url = row['url']\n",
    "        \n",
    "        # Parse the URL to extract the domain\n",
    "        parsed_url = urlparse(url)\n",
    "        onion_domain = parsed_url.netloc  # This gets the domain without http:// or trailing /\n",
    "\n",
    "        # Check if the domain ends with '.onion'\n",
    "        if onion_domain.endswith(\".onion\"):\n",
    "            # Write the full URL to the file\n",
    "            file.write(onion_domain + \"\\n\")\n",
    "\n",
    "print(f\"All .onion URLs have been written to {onion_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo proxychains nmap --top-ports 25 -sT -Pn -v --open -iL new_entries_onion_sites.txt -oX new_entries_nmap.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMAP XML to Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the output from NMAP is then converted into a Dataframe to be merged with the previous dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Nmap XML file\n",
    "xml_file = 'new_entries_nmap.xml'\n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse(xml_file)\n",
    "root = tree.getroot()\n",
    "\n",
    "# List to store data\n",
    "nmap_data_fixed = []\n",
    "\n",
    "# Iterate over each host in the Nmap XML output\n",
    "for host in root.findall('host'):\n",
    "    # Get the IP address\n",
    "    ip = host.find('address').get('addr')\n",
    "    \n",
    "    # Get the status (up/down)\n",
    "    status = host.find('status').get('state')\n",
    "    \n",
    "    # Initialize a variable to store the onion site (if found)\n",
    "    onion_site = None\n",
    "    \n",
    "    # Iterate through all hostname elements to find .onion domains\n",
    "    for hostname_elem in host.findall('hostnames/hostname'):\n",
    "        hostname = hostname_elem.get('name')\n",
    "        if hostname.endswith(\".onion\"):\n",
    "            onion_site = hostname  # Prioritize .onion hostname\n",
    "    \n",
    "    # Iterate over ports and extract relevant info\n",
    "    for port in host.findall('ports/port'):\n",
    "        port_id = port.get('portid')\n",
    "        protocol = port.get('protocol')\n",
    "        state = port.find('state').get('state')\n",
    "        service_name = port.find('service').get('name') if port.find('service') is not None else \"Unknown\"\n",
    "        \n",
    "        # Append the extracted data to the list, making sure we include the .onion site\n",
    "        nmap_data_fixed.append({\n",
    "            'Onion_Site': onion_site if onion_site else \"Unknown\",\n",
    "            'Port_Nmap': port_id,\n",
    "            'Protocol_Nmap': protocol,\n",
    "            'State_Nmap': state,\n",
    "            'Service_Nmap': service_name\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "df_nmap_fixed = pd.DataFrame(nmap_data_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_nmap_fixed and df_base using the 'Onion_Site' column\n",
    "df_merged = pd.merge(df_base, df_nmap_fixed, on='Onion_Site', how='inner')\n",
    "\n",
    "\n",
    "# Optionally, save the merged DataFrame to a CSV file\n",
    "df_merged.to_csv('merged_onion_nmap_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new and old merged results\n",
    "new_file = \"merged_onion_nmap_results.csv\"\n",
    "old_file = \"merged_onion_nmap_results_old.csv\"\n",
    "\n",
    "df_new = pd.read_csv(new_file)\n",
    "df_old = pd.read_csv(old_file)\n",
    "\n",
    "# Concatenate the old and new DataFrames\n",
    "df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "\n",
    "# Drop duplicate entries based on the 'Onion_Site' column (or other relevant column)\n",
    "df_combined = df_combined.drop_duplicates(subset='Onion_Site', keep='last')\n",
    "\n",
    "# Save the merged results back to a CSV file for future executions\n",
    "df_combined.to_csv(\"merged_onion_nmap_results_old.csv\", index=False)\n",
    "\n",
    "print(\"Updated results saved as merged_onion_nmap_results_old.csv for future executions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
